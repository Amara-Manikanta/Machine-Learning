{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Packages used in ML are:\n",
    "\n",
    "- Numpy\n",
    "- Scipy\n",
    "- Matplotlib\n",
    "- Pandas\n",
    "- Sckit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Machine Learning (ML) is broadly categorized into two main types: \n",
    "- <b>Supervised Learning</b> :\n",
    "    - The model is trained on a labeled dataset, which means that each training example is paired with an       output label. \n",
    "    - The goal is to learn a mapping from inputs to outputs.\n",
    "    - Types of Supervised Learning:\n",
    "        - <b>Regression</b>: \n",
    "            - The output variable is continuous and the goal is to predict a numerical value. \n",
    "            - Examples include predicting house prices, stock prices, etc.\n",
    "            - Example Algorithms: Linear Regression, Ridge Regression, Lasso Regression, Polynomial Regression.\n",
    "        - <b>Classification</b>: \n",
    "            - The output variable is categorical and the goal is to predict a category label. \n",
    "            - Examples include spam detection, image classification, etc.\n",
    "            - Example Algorithms: Logistic Regression, Support Vector Machines (SVM), Decision Trees, Random Forest, k-Nearest Neighbors (k-NN), Neural Networks.\n",
    "    - Regression algorithms :\n",
    "        - Ordinal regression\n",
    "        - Poisson regression\n",
    "        - Fast forest Quantile regression\n",
    "        - Linear, Polynomial, Lasso, Stepwise, Ridge regression\n",
    "        - Bayesian Linear regression\n",
    "        - Neural network regression\n",
    "        - Decision forest regression\n",
    "        - Boosted decision tree regression\n",
    "        - KNN (K-nearest neighbors)\n",
    "- <b>Unsupervised Learning </b>:\n",
    "    - The model is trained on data that does not have labeled responses. \n",
    "    - The goal is to infer the natural structure present within a set of data points.\n",
    "    - Types of Unsupervised Learning:\n",
    "        - <b>Clustering </b>: \n",
    "            - The goal is to group a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups.\n",
    "            - Grouping of data points or objects that are somehow similar by:\n",
    "                - Discovering structure\n",
    "                - Summerization\n",
    "                - Anomaly detection\n",
    "            - Example Algorithms: k-Means, Hierarchical Clustering, DBSCAN.\n",
    "        - <b>Dimensionality Reduction </b>: \n",
    "            - The goal is to reduce the number of random variables under consideration, by obtaining a set of principal variables.\n",
    "            - Example Algorithms: Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), Singular Value Decomposition (SVD).\n",
    "    - Unsupervised Learning teachniques are:\n",
    "        - Dimension reduction\n",
    "        - Density estimation\n",
    "        - Market basket analysis\n",
    "        - Clustering\n",
    "           \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffrence between Supervised and unsupervised \n",
    "|  | Supervised | Unsupervised |\n",
    "| ---- | --- | --- |\n",
    "| Labeled Data: | Requires labeled data. | Does not require labeled data. |\n",
    "|  | Classification : classifies labeled data | clustering : finds patterns and groupings from unlabeled data |\n",
    "| Goal: | Predict an output based on input data. | Discover the underlying structure or distribution in the data.|\n",
    "|  |Regression: predicts trends using previous labeled data | Has a fewer evaluation methods than supervised learning |\n",
    "|  | Has more evaluation methods than unsupervised learning. | Less Controlled environment.|\n",
    "|  | Controlled environment |\n",
    "|  Applications: | Suitable for tasks where historical data with labels is available.| Suitable for tasks where labels are not available, or we want to understand the structure of data. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Regression</b>\n",
    "- <b>Simple Linear Regression</b>\n",
    "    - Only one independent variable is used to estimate one dependent variable.\n",
    "    - we will have two variables:\n",
    "        - Y-dependent Variable -> this shoud be continous variable not a category type.\n",
    "        - X- Independent Variable\n",
    "    - y=Œ≤0+Œ≤1x+œµ\n",
    "        - y : The dependent variable (the outcome or response variable you are trying to predict).\n",
    "        - ùë• : The independent variable (the predictor or explanatory variable).\n",
    "        - ùõΩ0 : The intercept of the regression line (the value of ùë¶ when ùë• is 0).\n",
    "        - ùõΩ1 : The slope of the regression line (the change in y for a one-unit change in ùë•).\n",
    "        - ùúñ : The error term (the difference between the observed and predicted values of ùë¶).\n",
    "- <b>Multiple Linear Regression</b>\n",
    "    - When more than one independent variable is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training Accuracy</b>\n",
    "\n",
    "- Training accuracy is the accuracy of the model on the same dataset it was trained on. \n",
    "- It is calculated as the proportion of correctly predicted instances out of the total instances in the   training set. \n",
    "- High training accuracy indicates that the model is able to capture the patterns in the training data well. \n",
    "- However, very high training accuracy could also indicate overfitting, where the model has learned the noise in the training data rather than the underlying patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Out-of-Sample Accuracy</b>\n",
    "\n",
    "- Out-of-sample accuracy (also known as test accuracy) is the accuracy of the model on a dataset that was not used during training.\n",
    "- This dataset is called the test set or validation set. Out-of-sample accuracy is a better indicator of the model's performance on new, unseen data and helps to assess how well the model generalizes to other data.\n",
    "\n",
    "Importance of Out-of-Sample Accuracy\n",
    "- <b>Generalization</b>: High out-of-sample accuracy suggests that the model generalizes well to new data, which is the ultimate goal of most predictive models.\n",
    "- <b>Avoiding Overfitting</b>: Comparing training and out-of-sample accuracy helps detect overfitting. A large gap between training accuracy and out-of-sample accuracy typically indicates overfitting.\n",
    "- <b>Model Selection</b>: Out-of-sample accuracy is used to compare different models and select the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example on Out of accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.96\n",
      "Out-of-Sample Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "# Calculate training accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "# Calculate out-of-sample (test) accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Out-of-Sample Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>K-fold cross-validation</b>\n",
    "\n",
    "- K-fold cross-validation is a technique used in machine learning to evaluate the performance of a model in a more robust way than a simple train-test split. \n",
    "- It helps in assessing how the model will generalize to an independent dataset.\n",
    "\n",
    "How K-Fold Cross-Validation Works\n",
    "\n",
    "1. <b>Split the Data</b>: The entire dataset is randomly divided into k equal-sized subsets, or \"folds\".\n",
    "2. <b>Training and Validation</b>:\n",
    "    - For each fold, the model is trained using the remaining k-1 folds as the training data.\n",
    "    - The model is validated on the remaining single fold.\n",
    "3. <b>Repeat</b>: This process is repeated k times, with each fold used exactly once as the validation data.\n",
    "4. <b>Aggregate Results</b>: The results (e.g., accuracy) from each of the k iterations are averaged to produce a single performance metric.\n",
    "\n",
    "Benefits of K-Fold Cross-Validation\n",
    "\n",
    "- <b>Better Utilization of Data</b>: Every observation is used for both training and validation.\n",
    "- <b>Reduced Overfitting</b>: Since multiple train-test splits are used, the evaluation metric is more reliable.\n",
    "- <b>Generalization</b>: Provides a better indication of how the model will perform on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [1.         1.         0.93333333 0.96666667 0.96666667]\n",
      "Mean CV Score: 0.97\n",
      "Standard Deviation of CV Score: 0.02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Set up K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=kf)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Score: {cv_scores.mean():.2f}\")\n",
    "print(f\"Standard Deviation of CV Score: {cv_scores.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Evalutaion Matrix </b>\n",
    "\n",
    "1. <b> R - Squared (R¬≤) </b>: \n",
    "    - This metric indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. \n",
    "    - Higher values indicate a better fit.\n",
    "2. <b> Mean Absolute Error (MAE) </b>:\n",
    "    - The average of the absolute differences between predicted and actual values.\n",
    "3. <b> Mean Squared Error (MSE) </b>:\n",
    "    - The average of the squared differences between predicted and actual values.\n",
    "4. <b> Root Mean Squared Error (RMSE) </b> :\n",
    "    - The square root of the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
