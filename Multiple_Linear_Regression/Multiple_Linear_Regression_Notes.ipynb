{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Estimating Multiple Linear Regression Paramenters</b>\n",
    "\n",
    "1. <b>Ordinary Least Squares (OLS)</b>:\n",
    "    - The most common method for estimating the parameters of a multiple linear regression model.\n",
    "    - It minimizes the sum of the squared differences between the observed and predicted values.\n",
    "2. <b> Gradient Descent </b>:\n",
    "    - An iterative optimization algorithm used when the dataset is large or the number of predictors is high.\n",
    "    - It starts with an initial guess for the coefficients and updates them in the direction that reduces the cost function (mean squared error in the case of regression).\n",
    "3. <b> Least Absolute Shrinkage and Selection Operator (LASSO) </b>:\n",
    "    - A regularization method that adds a penalty equal to the absolute value of the magnitude of the coefficients to the cost function.\n",
    "    - This method can shrink some coefficients to zero, thus performing variable selection.\n",
    "4. <b> Ridge Regression</b>:\n",
    "    - Another regularization method that adds a penalty equal to the square of the magnitude of the coefficients to the cost function.\n",
    "    - This method is useful when there is multicollinearity among the predictor variables.\n",
    "5. <b>Elastic Net</b>:\n",
    "    - Combines the penalties of LASSO and Ridge regression.\n",
    "    - This method is useful when there are multiple correlated features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
